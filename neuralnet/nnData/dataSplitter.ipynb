{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9478c99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import html\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48dcb5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/aly/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26a62d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitSize = 80000\n",
    "filePath = '/Users/aly/Desktop/Main/funni_code/mldata/sentimentData.csv'\n",
    "\n",
    "\n",
    "for i, split in enumerate(pd.read_csv(filePath, chunksize = splitSize, encoding='ISO-8859-1')):\n",
    "    split.to_csv(f'testSplit{i}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd503fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit0.csv: ['NO_QUERY']\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit1.csv: ['NO_QUERY']\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit2.csv: ['NO_QUERY']\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit3.csv: ['NO_QUERY']\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit4.csv: ['NO_QUERY']\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit5.csv: ['NO_QUERY']\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit6.csv: ['NO_QUERY']\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit7.csv: ['NO_QUERY']\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit8.csv: ['NO_QUERY']\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit9.csv: ['NO_QUERY']\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit10.csv: ['NO_QUERY']\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit11.csv: ['NO_QUERY']\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit12.csv: ['NO_QUERY']\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit13.csv: ['NO_QUERY']\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit14.csv: ['NO_QUERY']\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit15.csv: ['NO_QUERY']\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit16.csv: ['NO_QUERY']\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit17.csv: ['NO_QUERY']\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit18.csv: ['NO_QUERY']\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit19.csv: ['NO_QUERY']\n"
     ]
    }
   ],
   "source": [
    "filePath = '/Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/'\n",
    "filePrefix = 'testSplit'\n",
    "fileCount = 20\n",
    "\n",
    "for i in range(fileCount):\n",
    "    fileName = f'{filePath}{filePrefix}{i}.csv'\n",
    "    if os.path.exists(fileName):\n",
    "        df = pd.read_csv(fileName, encoding = 'ISO-8859-1')\n",
    "        \n",
    "        if df.shape[1] >= 3:\n",
    "            uniqueVal = df.iloc[:,3].unique()\n",
    "            print(f'uniques - {fileName}: {uniqueVal}')\n",
    "        else:\n",
    "            print(f'{fileName} has <3 col')\n",
    "    else:\n",
    "        print(f'{fileName} DNE')\n",
    "\n",
    "##we know from this that all the files exist and that the loop works as intended\n",
    "##therefore, we don't need any more else statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72150121",
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = '/Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/'\n",
    "filePrefix = 'testSplit'\n",
    "fileCount = 20\n",
    "\n",
    "for i in range(fileCount):\n",
    "    fileName = f'{filePath}{filePrefix}{i}.csv'\n",
    "    df = pd.read_csv(fileName, encoding = 'ISO-8859-1')\n",
    "    df = df.iloc[:,[0,5]]\n",
    "    df.to_csv(fileName, index = False)\n",
    "    \n",
    "##clearing out all the extraneous columns (indices 1-4)\n",
    "##keeping [TARGET] and [MESSAGE] columns only :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76725319",
   "metadata": {},
   "outputs": [],
   "source": [
    "##now we need to replace all @ mentions in all files with generic \"@user\" for\n",
    "##the sake of efficiency\n",
    "\n",
    "filePath = '/Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/'\n",
    "filePrefix = 'testSplit'\n",
    "fileCount = 20\n",
    "\n",
    "for i in range(fileCount):\n",
    "    fileName = f'{filePath}{filePrefix}{i}.csv'\n",
    "    df = pd.read_csv(fileName, encoding = 'ISO-8859-1')\n",
    "    ##define @username pattern\n",
    "    userPattern = r'@\\w+'\n",
    "    df.iloc[:,1] = df.iloc[:,1].str.replace(userPattern, '@user', regex = True)\n",
    "    df.to_csv(fileName, index = False)\n",
    "    \n",
    "##all files should be ready to go..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc5db2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##now we will be removing all urls and encoding errors:\n",
    "\n",
    "def clean(line):\n",
    "    line = html.unescape(line)\n",
    "    line = re.sub(r'[^\\x00-\\x7F]+', ' ', line)\n",
    "    line = re.sub(r'http\\S+|www\\S+|https\\S+', '', line, flags = re.MULTILINE)\n",
    "    return line\n",
    "\n",
    "filePath = '/Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/'\n",
    "filePrefix = 'testSplit'\n",
    "fileCount = 20\n",
    "\n",
    "for i in range(fileCount):\n",
    "    fileName = f'{filePath}{filePrefix}{i}.csv'\n",
    "    df = pd.read_csv(fileName, encoding = 'ISO-8859-1')\n",
    "    df.iloc[:,1] = df.iloc[:,1].apply(clean)\n",
    "    df.to_csv(fileName, index = False)\n",
    "    \n",
    "##done yayyy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "442e3319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit0.csv: [0]\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit1.csv: [0]\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit2.csv: [0]\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit3.csv: [0]\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit4.csv: [0]\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit5.csv: [0]\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit6.csv: [0]\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit7.csv: [0]\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit8.csv: [0]\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit9.csv: [0 4]\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit10.csv: [4]\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit11.csv: [4]\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit12.csv: [4]\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit13.csv: [4]\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit14.csv: [4]\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit15.csv: [4]\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit16.csv: [4]\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit17.csv: [4]\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit18.csv: [4]\n",
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit19.csv: [4]\n"
     ]
    }
   ],
   "source": [
    "##checking if neutral (value of 2 in col index 0) exists\n",
    "filePath = '/Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/'\n",
    "filePrefix = 'testSplit'\n",
    "fileCount = 20\n",
    "\n",
    "for i in range(fileCount):\n",
    "    fileName = f'{filePath}{filePrefix}{i}.csv'\n",
    "    df = pd.read_csv(fileName, encoding = 'ISO-8859-1')\n",
    "    uniqueVal = df.iloc[:,0].unique()\n",
    "    print(f'uniques - {fileName}: {uniqueVal}')\n",
    "    \n",
    "##we know from this that all the files exist and that the loop works as intended\n",
    "##therefore, we don't need any more else statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90ea2a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uniques - /Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit19.csv: [0 4]\n"
     ]
    }
   ],
   "source": [
    "##checking source file for uniques\n",
    "\n",
    "filePath = '/Users/aly/Desktop/Main/funni_code/mldata/sentimentData.csv'\n",
    "df = pd.read_csv(filePath, encoding = 'ISO-8859-1')\n",
    "uniqueVal = df.iloc[:,0].unique()\n",
    "print(f'uniques - {fileName}: {uniqueVal}')\n",
    "\n",
    "##as 2 (neutral) doesnt exist in my cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a4bad53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg: 13.0\n"
     ]
    }
   ],
   "source": [
    "##checking average length of sentence (by word)\n",
    "\n",
    "filePath = '/Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/'\n",
    "filePrefix = 'testSplit'\n",
    "fileCount = 20\n",
    "##define globalavg\n",
    "globalAvg = 0\n",
    "\n",
    "##main loop\n",
    "for i in range(fileCount):\n",
    "    fileName = f'{filePath}{filePrefix}{i}.csv'\n",
    "    df = pd.read_csv(fileName, encoding = 'ISO-8859-1')\n",
    "    ##action\n",
    "    sentenceCol = df.iloc[:,1]\n",
    "    wordCount = sentenceCol.str.split().apply(len)\n",
    "    avg = wordCount.mean()\n",
    "    globalAvg += avg\n",
    "\n",
    "##get and print global avg length of sentence\n",
    "globalAvg /= 20\n",
    "print(f'avg: {globalAvg}')\n",
    "  \n",
    "## avg: 13.131260473709045\n",
    "## now to pad and cut :,) \n",
    "\n",
    "##ran this again after the next block to check if sentences are fixed length\n",
    "##the average is 13.0. they are fixed yay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "392f44e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "globalAvg = 13\n",
    "\n",
    "filePath = '/Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/'\n",
    "filePrefix = 'testSplit'\n",
    "fileCount = 20\n",
    "\n",
    "for i in range(fileCount):\n",
    "    fileName = f'{filePath}{filePrefix}{i}.csv'\n",
    "    df = pd.read_csv(fileName, encoding = 'ISO-8859-1')\n",
    "    \n",
    "    ##define sentence column\n",
    "    sentenceCol = df.iloc[:,1]\n",
    "\n",
    "    ##function to delete extra words\n",
    "    def fixedlength(sentence):\n",
    "        words = sentence.split()\n",
    "        if len(words) > globalAvg:\n",
    "            words = words[:globalAvg]\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    ##save df\n",
    "    df.iloc[:,1] = sentenceCol.apply(fixedlength)\n",
    "    df.to_csv(fileName, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f82316a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##removing all nan\n",
    "\n",
    "##filepath\n",
    "filePath = '/Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/'\n",
    "filePrefix = 'testSplit'\n",
    "fileCount = 20\n",
    "    \n",
    "for i in range(fileCount):\n",
    "    fileName = f'{filePath}{filePrefix}{i}.csv'\n",
    "    df = pd.read_csv(fileName, encoding = 'ISO-8859-1')\n",
    "    \n",
    "    df.dropna(inplace = True)\n",
    "\n",
    "    ##save df\n",
    "    df.to_csv(fileName, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b218e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "##urls not removed for some reason so we go again ig\n",
    "def removeURLfr(sentence):\n",
    "    urlPattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    return re.sub(urlPattern, ' ', sentence)\n",
    "\n",
    "##filepath\n",
    "filePath = '/Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/'\n",
    "filePrefix = 'testSplit'\n",
    "fileCount = 20\n",
    "\n",
    "for i in range(fileCount):\n",
    "    fileName = f'{filePath}{filePrefix}{i}.csv'\n",
    "    df = pd.read_csv(fileName, encoding='ISO-8859-1')\n",
    "    \n",
    "    ##col ind1\n",
    "    sentenceCol = df.iloc[:, 1]\n",
    "    \n",
    "    df.iloc[:, 1] = sentenceCol.apply(removeURLfr)\n",
    "    \n",
    "    df.to_csv(fileName, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a8c89dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##remove all special characters and uppercase\n",
    "\n",
    "def removeBad(sentence):\n",
    "    specialChar = string.punctuation.replace('@', '')\n",
    "    key = ' ' * len(specialChar)\n",
    "    translator = str.maketrans(specialChar, key)\n",
    "    cleaned = sentence.translate(translator).lower()\n",
    "    return cleaned\n",
    "\n",
    "##function to pad or delete extra words\n",
    "def fixedlength(sentence):\n",
    "    words = sentence.split()\n",
    "    if len(words) < globalAvg:\n",
    "        words.extend(['<PAD>'] * (globalAvg - len(words)))\n",
    "    elif len(words) > globalAvg:\n",
    "        words = words[:globalAvg]\n",
    "    return ' '.join(words)\n",
    "\n",
    "for i in range(fileCount):\n",
    "    fileName = f'{filePath}{filePrefix}{i}.csv'\n",
    "    df = pd.read_csv(fileName, encoding='ISO-8859-1')\n",
    "    \n",
    "    ##col ind1\n",
    "    sentenceCol = df.iloc[:, 1]\n",
    "    \n",
    "    df.iloc[:,1] = sentenceCol.apply(removeBad)\n",
    "    df.iloc[:,1] = sentenceCol.apply(fixedlength)\n",
    "    df.to_csv(fileName, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9aa7c9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg: 13.0\n"
     ]
    }
   ],
   "source": [
    "##checking global average length of sentence again\n",
    "\n",
    "filePath = '/Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/'\n",
    "filePrefix = 'testSplit'\n",
    "fileCount = 20\n",
    "globalAvg = 0\n",
    "\n",
    "for i in range(fileCount):\n",
    "    fileName = f'{filePath}{filePrefix}{i}.csv'\n",
    "    df = pd.read_csv(fileName, encoding = 'ISO-8859-1')\n",
    "    ##action\n",
    "    sentenceCol = df.iloc[:,1]\n",
    "    wordCount = sentenceCol.str.split().apply(len)\n",
    "    avg = wordCount.mean()\n",
    "    globalAvg += avg\n",
    "    \n",
    "globalAvg /= 20\n",
    "print(f'avg: {globalAvg}')\n",
    "\n",
    "##we get 13.0, which is our target :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5c57529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e93d22e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##sorting 0-10000 for dataset 0 and 19 randomly, as it contains both +/- sentiment\n",
    "\n",
    "filePath1 = '/Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit0.csv'\n",
    "filePath2 = '/Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/testSplit19.csv'\n",
    "df1 = pd.read_csv(filePath1)\n",
    "df2 = pd.read_csv(filePath2)\n",
    "\n",
    "\n",
    "df1Head = df1.head(10000)\n",
    "df2Head = df2.head(10000)\n",
    "\n",
    "df = pd.concat([df1Head, df2Head], axis = 0)\n",
    "\n",
    "df = df.sample(frac = 1, random_state = 7)\n",
    "\n",
    "newFilePath = '/Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/fixedRandom/randomBasic.csv'\n",
    "\n",
    "df.to_csv(newFilePath, index = False, header = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "28617799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "modelPath = '/Users/aly/Desktop/Main/funni_code/mldata/glove.6B.50d.txt'\n",
    "wordVec = KeyedVectors.load_word2vec_format(modelPath, binary=False, no_header = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c5e88881",
   "metadata": {},
   "outputs": [],
   "source": [
    "##now we must embed every sentence in our new dataset\n",
    "##wordvec\n",
    "#modelPath = '/Users/aly/Desktop/Main/funni_code/mldata/glove.6B.50d.txt'\n",
    "#wordVec = KeyedVectors.load_word2vec_format(modelPath, binary=False, no_header = True)\n",
    "\n",
    "def sentenceEmbed(sentence, wordVec):\n",
    "    words = sentence.split()\n",
    "    wordEmbedding = [wordVec[word] for word in words if word in wordVec]\n",
    "    if len(wordEmbedding) == 0:\n",
    "        return None\n",
    "    sentenceEmbed = np.mean(wordEmbedding, axis=0)\n",
    "    return sentenceEmbed\n",
    "\n",
    "filePath = '/Users/aly/Desktop/Main/funni_code/machinelearning/neuralnet/nnData/fixedRandom/randomBasic.csv'\n",
    "df = pd.read_csv(filePath)\n",
    "\n",
    "df.iloc[:,1] = df.iloc[:,1].apply(lambda x: sentenceEmbed(x, wordVec))\n",
    "\n",
    "df.to_csv(filePath, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
